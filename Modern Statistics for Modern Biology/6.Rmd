---
title: '6'
output: html_document
date: "2023-04-10"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 6 Testing

In this chapter we will:

 -  Familiarize ourselves with the statistical machinery of hypothesis testing, its vocabulary, its purpose, and its strengths and limitations.

 - Understand what multiple testing means.

 - See that multiple testing is not a problem – but rather, an opportunity, as it overcomes many of the limitations of single testing.

 - Understand the false discovery rate.

 - Learn how to make diagnostic plots.

 - Use hypothesis weighting to increase the power of our analyses.
 
Suppose we measured the expression level of a marker gene to decide whether some cells we are studying are from cell type A or B. First, let’s consider that we have no prior assumption, and it’s equally important to us to get the assignment right no matter whether the true cell type is A or B. This is a classification task. We’ll cover classification in Chapter 12. In this chapter, we consider the asymmetric case: based on what we already know (we could call this our prior knowledge), we lean towards conservatively calling any cell A, unless there is strong enough evidence for the alternative. Or maybe class B is interesting, rare, and/or worthwhile studying further, whereas A is a “catch-all” class for all the boring rest. In such cases, the machinery of hypothesis testing is for us. 
 
 
```{r}
## ----initialize, echo = FALSE, message = FALSE, error = FALSE-----------------
#source("../chapter-setup.R"); chaptersetup("/__w/CUBook/CUBook/Chap14-MultipleTest/multtest.Rnw", "14")
#knitr::opts_chunk$set(dev = 'pdf', dpi = 300, fig.margin = TRUE, fig.show = 'hold', fig.keep = 'none')


## ---- xkcdmulttest-newspapertitle, eval = TRUE, echo = FALSE, fig.show = 'hold', fig.keep = 'high'----
#knitr::include_graphics(c('images/xkcdmulttest-newspapertitle.png'), dpi = NA)


## ---- active-substance-discovery-robot-screening-robot, eval = TRUE, echo = FALSE, fig.show = 'hold', fig.keep = 'high', fig.cap = "High-throughput data in modern biology are screened for associations with millions of hypothesis tests. ([Source: Bayer](https://www.research.bayer.com/en/automated-search-for-active-ingredients-with-robots.aspx))"----
#knitr::include_graphics(c('images/active-substance-discovery-robot-screening-robot.jpg'), dpi = NA)


## ----FDRsetup, message = FALSE, echo = FALSE----------------------------------
library("tibble")
library("dplyr")
library("ggplot2")
library("gganimate")
library("magick")
require("transformr")
```


```{r}
## ----FDRcomputestuff, echo = FALSE--------------------------------------------
makedata = function(px, f1, f2, pi0, xcut, what) {
  stopifnot(length(px)==length(f1), 
            length(px)==length(f2), 
            length(pi0)==1, 
            length(xcut)==1,
            length(what)==1,
            pi0>=0, pi0<=1)
  f1 = f1 * pi0
  f2 = f2 * (1-pi0)
  i1 = which(px >= xcut)
  i2 = seq(1, i1[1], by = 1L)
  maxf1 = max(f1)
  maxf2 = max(f2)
  bind_rows(  
  tibble(
    x = px[c(i1, rev(i1))],
    y = c(f1[i1], rep(0, length(i1))),
    outcome = "True Negative"),
  tibble(
    x = px[c(i2, rev(i2))],
    y = c(f1[i2], rep(0, length(i2))),
    outcome = "False Positive"),
  tibble(
    x = px[c(i1, rev(i1))],
    y = c(f2[i1], rep(0, length(i1))) + maxf1,
    outcome = "False Negative"),
  tibble(
    x = px[c(i2, rev(i2))],
    y = c(f2[i2], rep(0, length(i2))) + maxf1,
    outcome = "True Positive"),
  tibble(
    x = rep(xcut, 3L),
    y = c(0, maxf1+maxf2, 0),
    outcome = ""
  )) |>
  bind_cols(tibble(xcut = xcut, what = what))
} 

findclosest = function(x, x0) {x[which.min(abs(x-x0))]}

pi0 = 2/3
t_df = 4
pxa = seq(-4, 4, length.out = 500)
pxb = pt(pxa, df = t_df)
xcuta = findclosest(pxa, qt(0.05, df = t_df))
xcutb = findclosest(pxb,    0.05)
f1a = dt(pxa, df = t_df) 
f2a = dgamma(pxa + 4, shape = 2, rate = 0.8) 

chainrulefac = (diff(pxa)/diff(pxb)) |> {\(x) c(x, last(x))}()
f1b = f1a * chainrulefac |> {\(x) x/sum(x)}()
f2b = f2a * chainrulefac |> {\(x) x/sum(x)}()
f1b = f1b/sum(f1b)
f2b = f2b/sum(f2b)

df = bind_rows(
  makedata(pxa, f1a, f2a, pi0, xcuta, "x"),
  makedata(pxb, f1b, f2b, pi0, xcutb, "p-value")
) 

make_static_plot = function(df) {
  stopifnot(nrow(df)>=3)
  colpal = setNames(
      c(RColorBrewer::brewer.pal(12, "Paired")[c(6,5,1,2)], "black"),
      c("True Positive", "False Negative",
        "False Positive", "True Negative", ""))
  ggplot(df, aes(x = x, y = y, fill = outcome, col = outcome)) + 
    geom_polygon() +
    scale_fill_manual(values = colpal) +
    scale_colour_manual(values = colpal) +
    xlab("value") +
    theme(legend.position = "bottom",
          legend.title = element_blank(), 
          axis.title.y = element_blank(), 
          axis.text.y  = element_blank(),
          axis.ticks.y = element_blank())
}

```


```{r}

## ----mt-FDRvspstatic1, fig.keep = 'high', fig.cap = "Making a binary (yes/no) decision. Here, we call the two possible decisions \"positive\" and \"negative\" based on some continuous-valued score $x$, shown along the $x$-axis. The curve shaded in blue shows the distribution density of $x$ for one of the classes (the negatives), the curve shaded in red, for the other class (the positives). The distributions are distinctive (the red values are generally lower), but have some overlap. The vertical black bar marks some choice of a decision boundary, which results in four possible outcomes highlighted by the color key.", fig.width = 5.6, fig.height = 3, echo = FALSE----
make_static_plot(dplyr::filter(df, what == "x"))

```

```{r}


## ----mt-FDRvspstatic2, fig.keep = 'high', fig.cap = "Analogous to Figure \\@ref(fig:mt-FDRvspstatic1), but now we have transformed $x$ from its original range to the range $[0,1]$ using a non-linear, strictly increasing transformation function $p=f(x)$, which we chose such that the resulting blue distribution is uniform. Such a function always exists: it is the cumulative distribution function of $x$ (we have seen it in Section \\@ref(rgraphics:sub:ecdf)). We call the result a **p-value**. The definition of the FDR in Equation \\@ref(eq:mt-simplefdr) applies equally well in Figure \\@ref(fig:mt-FDRvspstatic1) and here.", fig.width = 5.6, fig.height = 3, echo = FALSE----
make_static_plot(dplyr::filter(df, what == "p-value"))

```

Note that this definition does not require the concept or even the calculation of a p-value. It works for any arbitrarily defined score. However, it requires knowledge of three things:

 - the distribution of in the blue class (the blue curve),

 - the distribution of in the red class (the red curve),

 - the relative sizes of the blue and the red classes.

If we know these, then we are basically done at this point; or we can move on to supervised classification in Chapter 12, which deals with the extension of Figure 6.2 to multivariate.

Very often, however, we do not know all of these, and this is the realm of hypothesis testing. In particular, suppose that one of the two classes (say, the blue one) is easier than the other, and we can figure out its distribution, either from first principles or simulations. We use that fact to transform our score to a standardized range between 0 and 1 (see Figures 6.2—6.4), which we call the p-value. We give the class a fancier name: null hypothesis. We do not insist on knowing Item 2 (and we give another fancy name, alternative hypothesis, to the red class). As for Item 3, we can use the conservative upper limit that the null hypothesis is far more prevalent (or: likely) than the alternative and do our calculations under the condition that the null hypothesis is true. This is the situation we are going to explore in the rest of this chapter.

Thus, instead of basing our decision-making on the FDR, we base it on the 

$$\text{FDR}=\frac{\text{area shaded in light blue}}{\text{light blue + strong red}}$$

$$p \text{ value}=\frac{\text{area shaded in light blue}}{\text{overall blue area}}$$

In other words, the p-value is the precise and often relatively easy-to-compute answer to the wrong question; whereas the FDR answers the right question, but requires a lot more input, which we do not always have.

```{r}


## ----mt-FDRvspanim, fig.keep = 'high', fig.cap = "The animation highlights the analogies between using a generic score $x$ (as in Fig. \\@ref(fig:mt-FDRvspstatic1)) and a p-value from a formal hypothesis test (as in Fig. \\@ref(fig:mt-FDRvspstatic2)) for decision making. We will come back to these concepts in terms of the two-group model in Section \\@ref(sec:mt:localfdr) and Figure \\@ref(fig:mt-lfdr).", fig.width = 5.6, fig.height = 3, echo = FALSE, fig.show = "asis"----
p1 <- make_static_plot(df) + 
  labs(title = "{closest_state}") +
  transition_states(what,
                    transition_length = 3,
                    state_length = 1) + 
  view_follow() +
  ease_aes("cubic-in-out")
animate(p1, renderer = magick_renderer(), width = 5.6, height = 3, units = "in", res = 150, device = "png")
```


So now let’s dive into hypothesis testing, starting with single testing. To really understand the mechanics, we use one of the simplest possible examples: suppose we are flipping a coin to see if it is fair. We flip the coin 100 times and each time record whether it came up heads or tails. So, we have a record that could look something like this: 

`HHTTHTHTT…` 

which we can simulate in R. Let’s assume we are flipping a biased coin, so we set probHead different from 1/2:


```{r}



## ----whatprob1----------------------------------------------------------------
set.seed(0xdada)
numFlips = 100
probHead = 0.6
coinFlips = sample(c("H", "T"), size = numFlips,
  replace = TRUE, prob = c(probHead, 1 - probHead))
head(coinFlips)

```

```{r}


## ----tableCoinFlips-----------------------------------------------------------
table(coinFlips)

```


```{r}


## ----binomDens----------------------------------------------------------------
library("dplyr")
k = 0:numFlips
numHeads = sum(coinFlips == "H")
binomDensity = tibble(k = k,
     p = dbinom(k, size = numFlips, prob = 0.5))

```


```{r}


## ----mt-dbinom, fig.keep = 'high', fig.cap = "The binomial distribution for the parameters $n=(ref:mt-dbinom-1)$ and $p=0.5$, according to Equation \\@ref(eq:mt-dbinom).", fig.width = 3.5, fig.height = 3----
library("ggplot2")
ggplot(binomDensity) +
  geom_bar(aes(x = k, y = p), stat = "identity") +
  geom_vline(xintercept = numHeads, col = "blue")

```


We can still use Monte Carlo simulation to give us something to compare with:


```{r}


## ----rbinom, fig.keep = 'high', fig.cap = "An approximation of the binomial distribution from $(ref:rbinom-1)$ simulations (same parameters as Figure \\@ref(fig:mt-dbinom)).", fig.dim = c(3.5, 3)----
numSimulations = 10000
outcome = replicate(numSimulations, {
  coinFlips = sample(c("H", "T"), size = numFlips,
                     replace = TRUE, prob = c(0.5, 0.5))
  sum(coinFlips == "H")
})
ggplot(tibble(outcome)) + xlim(-0.5, 100.5) +
  geom_histogram(aes(x = outcome), binwidth = 1, center = 50) +
  geom_vline(xintercept = numHeads, col = "blue")
```

```{r}

## ----mt-findrej, fig.keep = 'high', fig.cap = "As Figure \\@ref(fig:mt-dbinom), with rejection region (red) that has been chosen such that it contains the maximum number of bins whose total area is at most $\\alpha=(ref:mt-findrej-1)$.", fig.width=3.5, fig.height=3----
library("dplyr")
alpha = 0.05
binomDensity = arrange(binomDensity, p) |>
        mutate(reject = (cumsum(p) <= alpha))

ggplot(binomDensity) +
  geom_bar(aes(x = k, y = p, col = reject), stat = "identity") +
  scale_colour_manual(
    values = c(`TRUE` = "red", `FALSE` = "darkgrey")) +
  geom_vline(xintercept = numHeads, col = "blue") +
  theme(legend.position = "none")
```


We have just gone through the steps of a binomial test. In fact, this is such a frequent activity in R that it has been wrapped into a single function, and we can compare its output to our results.


```{r}



## ----assertion, echo=FALSE----------------------------------------------------
#stopifnot( .tmp1 + .tmp2 != numFlips )


## ----binom.test---------------------------------------------------------------
binom.test(x = numHeads, n = numFlips, p = 0.5)

```


```{r}


## ---- typesoferror, echo = FALSE----------------------------------------------
dat <- data.frame(c('**Reject null hypothesis**', '**Do not reject**'),
                          c('Type I error (false positive)', 'True negative'),
                          c('True positive', 'Type II error (false negative)'))
            knitr::kable(dat, col.names = c('Test vs reality', 'Null hypothesis is true', '$...$ is false'), caption = 'Types of error in a statistical test.')
```

```{r}


## ----checkbyexperimentalmaths, echo=FALSE, results="hide"---------------------
.myttest = function(x, y) {
  mx  = mean(x)
  my  = mean(y)
  s12 = sqrt((sum((x-mx)^2)+sum((y-my)^2)) / (length(x)+length(y)-2))
  (mx - my) / s12 * sqrt(length(x)*length(y)/(length(x)+length(y)))
}
replicate(100, {
  x = rnorm(ceiling(30 * runif(1)))
  y = rnorm(ceiling(30 * runif(1)))
  stopifnot(abs(.myttest(x, y) - t.test(x, y, var.equal=TRUE)$statistic) < 1e-9)
})

```

Suppose we want to compare the difference between means of samples selected from two populations (the treatment and control). Assume both groups have normally distributed observations. Then $$Z = \frac{(\bar{X}_{t}- \bar{X}_{c})-(\mu_{t}-\mu_{c})}{\sqrt{\left(\frac{\sigma^{2}_{t}}{n_t}+ \frac{\sigma^{2}_{c}}{n_c} \right)}}$$


Suppose that $\sigma_{t}^{2}$ and $\sigma_{c}^{2}$ are unknown but can be assumed equal to $\sigma^2$. The pooled estimate $S_{p}^{2}$ for $\sigma^2$ equal to $$S_{p}^{2} = \frac{S_{t}^{2}(n_{t}-1)+ S_{c}^{2}(n_{c}-1)}{[n_t+n_c-2]}$$ where $S_{t}^2$ and $S_{c}^2$ are the sample estimates of the treatment and control groups. 


Many experimental measurements are reported as rational numbers, and the simplest comparison we can make is between two groups, say, cells treated with a substance compared to cells that are not. The basic test for such situations is the $t$-test. The test statistic is defined as

$$t=c\frac{m_1-m_2}{s}$$ where $m_1$ and $m_2$ are the mean of the values in the two groups, $s$ is the pooled standard deviation and $c$ is a constant that depends on the sample sizes, i.e., the numbers of observations $m_1$ and $m_2$ in the two groups. In formulas 

$$m_g=\frac{1}{n_g}\sum_{i=1}^{n_g}x_{g,i}\quad g=1,2$$ 

$$s^2=\frac{1}{n_1+n_2-2}\left(\sum_{i=1}^{n_1}(x_{1,i}-m_1)^2+\sum_{j=1}^{n_2}(x_{2,j}-m_2)^2\right)$$

$$c=\sqrt{\frac{n_1n_2}{n_1+n_2}}$$

```{r}


## ----mt-plantgrowth, fig.keep = 'high', fig.cap = "The `PlantGrowth` data.\\label{mt-plantgrowth}", fig.width = 3, fig.height = 2.75----
library("ggbeeswarm")
data("PlantGrowth")
ggplot(PlantGrowth, aes(y = weight, x = group, col = group)) +
  geom_beeswarm() + theme(legend.position = "none")
tt = with(PlantGrowth,
          t.test(weight[group =="ctrl"],
                 weight[group =="trt2"],
                 var.equal = TRUE))
```

```{r}
tt
```


```{r}


## ----mt-ttestperm, fig.keep = 'high', fig.cap = "The null distribution of the (absolute) $t$-statistic determined by simulations -- namely, by random permutations of the group labels.\\label{mt-ttestperm}", fig.width = 3, fig.height = 2.75----
abs_t_null = with(
  dplyr::filter(PlantGrowth, group %in% c("ctrl", "trt2")),
    replicate(10000,
      abs(t.test(weight ~ sample(group))$statistic)))

ggplot(tibble(`|t|` = abs_t_null), aes(x = `|t|`)) +
  geom_histogram(binwidth = 0.1, boundary = 0) +
  geom_vline(xintercept = abs(tt$statistic), col = "red")
```


```{r}


mean(abs(tt$statistic) <= abs_t_null)

```

```{r}


## ----tttestpermcheck, echo = FALSE--------------------------------------------
stopifnot(abs(mean(abs(tt$statistic) <= abs_t_null) -  tt$p.value) < 0.0025)


## ----ttdup--------------------------------------------------------------------
with(rbind(PlantGrowth, PlantGrowth),
       t.test(weight[group == "ctrl"],
              weight[group == "trt2"],
              var.equal = TRUE))
```

```{r}


## ----prohHead_assertion, echo=FALSE-------------------------------------------
stopifnot(probHead!=0.5)


## ---- mterrors, echo = FALSE--------------------------------------------------
dat <- data.frame(c('**Rejected**', '**Not rejected**', '**Total**'),
                              c('$V$', '$U$', '$m_0$'),
                              c('$S$', '$T$','$m-m_0$'),
                              c('$R$', '$m-R$', '$m$'))
            knitr::kable(dat, col.names = c('Test vs reality', 'Null hypothesis is true', '$...$ is false', 'Total'), caption = 'Types of error in multiple testing. The letters designate the number of
    times each type of error occurs.')
```

```{r}

## ----typeerror3---------------------------------------------------------------
1 - (1 - 1/1e6)^8e5
```

```{r}

## ----mt-bonferroni, fig.keep = 'high', fig.cap = "Bonferroni method. The plot shows the graph of \\@ref(eq:mt-bonferroni) for $m=(ref:mt-bonferroni-1)$ as a function of $\\alpha$.", fig.width = 3, fig.height = 2.75----
m = 10000
ggplot(tibble(
  alpha = seq(0, 7e-6, length.out = 100),
  p     = 1 - (1 - alpha)^m),
  aes(x = alpha, y = p)) +  geom_line() +
  xlab(expression(alpha)) +
  ylab("Prob( no false rejection )") +
  geom_hline(yintercept = 0.05, col = "red")
```

```{r}

## ----mtdeseq2airway, message=FALSE--------------------------------------------
library("DESeq2")
library("airway")
library("ggplot2")
data("airway")
airway
colnames(airway)
colData(airway)

```

```{r}
aw   = DESeqDataSet(se = airway, design = ~ cell + dex)
aw   = DESeq(aw)
results(aw)

```

```{r}
awde = as.data.frame(results(aw)) |> dplyr::filter(!is.na(pvalue))
awde
```

```{r}
## ----mt-awpvhist, fig.keep = 'high', fig.cap = "p-value histogram of for the `airway` data.", fig.width = 3, fig.height = 2.75----
ggplot(awde, aes(x = pvalue)) +
  geom_histogram(binwidth = 0.025, boundary = 0)
```


```{r}


## ----mt-awpvvisfdr, fig.keep = 'high', fig.cap = "Visual estimation of the FDR with the p-value histogram.", fig.width = 3, fig.height = 2.75----
alpha = binw = 0.025
pi0 = 2 * mean(awde$pvalue > 0.5)
pi0

```

```{r}
nrow(awde)
pi0 = 2 * mean(awde$pvalue > 0.5)
sum(awde$pvalue <= alpha) # number of genes with p value less than alpha
pi0 * binw * nrow(awde)
mean(awde$pvalue <= alpha) # fraction of genes with p value less than alpha
pi0 * alpha
```

```{r}
ggplot(awde,
  aes(x = pvalue)) + geom_histogram(binwidth = binw, boundary = 0) +
  geom_hline(yintercept = pi0 * binw * nrow(awde), col = "blue") +
  geom_vline(xintercept = alpha, col = "red")
```

```{r}

## ----fdrvis-------------------------------------------------------------------
pi0 * alpha / mean(awde$pvalue <= alpha)

```

```{r}
library("dplyr")
## ----mt-BH, fig.keep = 'high', fig.cap = "Visualization of the Benjamini-Hochberg procedure. Shown is a zoom-in to the 7000 lowest p-values.\\label{mt-BH}", fig.width = 3, fig.height = 2.75----
phi  = 0.10
awde = mutate(awde, rank = rank(pvalue))
m    = nrow(awde)

ggplot(dplyr::filter(awde, rank <= 7000), aes(x = rank, y = pvalue)) +
  geom_line() + geom_abline(slope = phi / m, col = "red")
```

```{r}
## ----kmax---------------------------------------------------------------------
kmax = with(arrange(awde, rank),
         last(which(pvalue <= phi * rank / m)))
kmax

```


```{r}
library("dplyr")
## ----mt-SchwederSpjotvoll, fig.keep = 'high', fig.cap = "Schweder and Spj\\o{}tvoll plot, as described in the answer to Question \\@ref(ques:Testing-ques-SchwederSpjotvoll).", fig.dim = c(4.5, 4.5)----
awdef = awde %>%
  dplyr::filter(baseMean >=1) %>% 
  arrange(pvalue) %>%
  mutate(oneminusp = 1 - pvalue,
         N = n() - row_number())
awdef

```

```{r}
library("ggplot2")
jj = round(nrow(awdef) * c(1, 0.5))
jj
diff(awdef$N[jj])
diff(awdef$oneminusp[jj])

```

```{r}
slope = with(awdef, diff(N[jj]) / diff(oneminusp[jj]))
slope
```

There are 22853 rows in `awdef`, thus, according to this simple estimate, there are 22853-17302=5551 alternative hypotheses.

```{r}
ggplot(awdef) +
  geom_point(aes(x = oneminusp, y = N), size = 0.15) + 
  xlab(expression(1-p[i])) +
  ylab(expression(N(p[i]))) +
  geom_abline(intercept = 0, slope = slope, col = "red3") +
  geom_hline(yintercept = slope, linetype = "dotted") +
  geom_vline(xintercept = 1, linetype = "dotted") +
  geom_text(x = 0, y = slope, label = paste(round(slope)), 
            hjust = -0.1, vjust = -0.25) 
```

```{r}


## ---- mt-sunexplode, fig.margin = FALSE, eval = TRUE, echo = FALSE, fig.show = 'hold', fig.keep = 'high', fig.cap = "From [http://xkcd.com/1132](http://xkcd.com/1132) -- While the frequentist only has the currently available data, the Bayesian can draw on her understanding of the world or on previous experience. As a Bayesian, she would know enough about physics to understand that our sun\'s mass is too small to become a nova. Even if she does not know physics, she might be an **empirical Bayesian** and draw her prior from a myriad previous days where the sun did not go nova."----
#knitr::include_graphics(c('images/xkcd1132.png'), dpi = NA)


## ----mt-lfdr, fig.keep = 'high', fig.cap = "Local false discovery rate and the two-group model, with some choice of $f_{\\text{alt}}(p)$, and $\\pi_0=(ref:mt-lfdr-1)$. Top: densities. Bottom: distribution functions.", fig.width = 3, fig.height = 6, echo = FALSE----
pi0 = 0.6
f1 = function(t, shape2 = 7) {
   rv = dbeta(t, 1, shape2)
   rv / sum(rv) * (length(rv)-1) * (1-pi0)
}

t = seq(0, 1, length.out = 101)
t0 = 0.1

f0  = rep(pi0, length(t))
f   = f0 + f1(t)
F0  = cumsum(f0) / (length(t)-1)
F   = cumsum(f)  / (length(t)-1)
stopifnot(abs(F[length(F)] - 1) < 1e-2)
```


```{r}

myplot = function(y, y0, ylim, yat, havepi0, colo = RColorBrewer::brewer.pal(12, "Paired")) {
  plot(x = t, y = y, type = "l", xlim = c(0, 1), ylim = ylim,
    xaxs = "i", yaxs = "i", ylab = "", yaxt = "n", xaxt = "n", xlab = "", main = deparse(substitute(y)))
  axis(side = 1, at = c(0, 1))
  axis(side = 2, at = yat)
  xa  =  t[t<=t0]
  xb  =  t[t>=t0]
  y0a = y0[t<=t0]
  y0b = y0[t>=t0]
  ya  =  y[t<=t0]
  polygon(x = c(xa, rev(xa)), y = c(y[t<=t0], rev(y0a)), col = colo[2])
  polygon(x = c(xb, rev(xb)), y = c(y[t>=t0], rev(y0b)), col = colo[1])
  polygon(x = c(xa, rev(xa)), y = c(rep(0, length(xa)), rev(y0a)), col = "#c0c0c0")
  polygon(x = c(xb, rev(xb)), y = c(rep(0, length(xb)), rev(y0b)), col = "#f0f0f0")
  segments(x0 = rep(t0, 2), x1 = rep(t0, 2), y0 = c(0, last(y0a)), y1 = c(last(y0a), last(ya)),
           col = colo[5:6], lwd = 3)
  text(t0, 0, adj = c(0, 1.8), labels = expression(p), cex = 1, xpd = NA)
  if (havepi0)
      text(0, pi0, adj = c(1.5, 0.5), labels = expression(pi[0]), cex = 1, xpd = NA)
}

par(mai = c(1, 0.6, 0.4, 0.3), mfcol = c(2,1))
myplot(f, f0, ylim = c(0, f[1]), yat = c(0:3),       havepi0 = TRUE)
myplot(F, F0, ylim = c(0, 1),    yat = c(0, 0.5, 1), havepi0 = FALSE)
```


```{r}

## ----fdrtool, fig.width = 4, fig.height = 8, message = FALSE, results = "hide"----
library("fdrtool")
ft = fdrtool(awde$pvalue, statistic = "pvalue")


```

```{r}

## ----qvalue31-----------------------------------------------------------------
ft$param[,"eta0"]

```

```{r}


## ----awde_basemean_counts-----------------------------------------------------
awde$baseMean[1]
cts = counts(aw, normalized = TRUE)[1, ]
cts
mean(cts)

```

asinh: Inverse hyperbolic functions 

Inverse hyperbolic sine (a.k.a. area hyperbolic sine) (Latin: Area sinus hyperbolicus):
$${\displaystyle \operatorname {arsinh} x=\ln \left(x+{\sqrt {x^{2}+1}}\right)}$$




```{r}

## ----makesure, echo=FALSE-----------------------------------------------------
stopifnot(abs(mean(cts)-awde$baseMean[1])<1e-9)


## ----mt-basemean-hist, fig.keep = 'high', fig.cap = "Histogram of `baseMean`. We see that it covers a large dynamic range, from close to 0 to around (ref:mt-basemean-hist-1). \\label{mt-basemean-hist}", fig.width = 3, fig.height = 2.4----
ggplot(awde, aes(x = asinh(baseMean))) +
  geom_histogram(bins = 60)

```

```{r}
## ----mt-basemean-hist, fig.keep = 'high', fig.cap = "Histogram of `baseMean`. We see that it covers a large dynamic range, from close to 0 to around (ref:mt-basemean-hist-1). \\label{mt-basemean-hist}", fig.width = 3, fig.height = 2.4----
ggplot(awde, aes(x = baseMean)) +
  geom_histogram(bins = 60)
```

```{r}
ggplot(awde, aes(x = log2(baseMean+1))) +
  geom_histogram(bins = 60)
```

```{r}

## ----mt-basemean-scp, fig.keep = 'high', fig.cap = "Scatterplot of the rank of `baseMean` versus the negative logarithm of the p-value. For small values of `baseMean`, no small p-values occur. Only for genes whose read counts across all observations have a certain size, the test for differential expression has power to come out with a small p-value. \\label{mt-basemean-scp}", fig.width = 3, fig.height = 2.4----
ggplot(awde, aes(x = rank(baseMean), y = -log10(pvalue))) +
  geom_hex(bins = 60) +
  theme(legend.position = "none")

```

```{r}

## ----awde_stratify------------------------------------------------------------
awde = mutate(awde, stratum = cut(baseMean, include.lowest = TRUE,
  breaks = signif(quantile(baseMean,probs=seq(0,1,length.out=7)),2)))

```

```{r}

## ----mt-awde-stratified-hist, fig.keep = 'high', fig.cap = "p-value histograms of the airway data, stratified into equally sized groups defined by increasing value of `baseMean`. \\label{mt-awde-stratified-hist}", fig.width = 2.8, fig.height = 4----
ggplot(awde, aes(x = pvalue)) + facet_wrap( ~ stratum, nrow = 4) +
  geom_histogram(binwidth = 0.025, boundary = 0)

```

```{r}

## ----mt-awde-stratified-ecdf, fig.keep = 'high', fig.cap = "Same data as in #Figure \\@ref(fig:mt-awde-stratified-hist), shown with ECDFs.\\label{mt-awde-stratified-ecdf}", #fig.width = 4, fig.height = 3.4----
ggplot(awde, aes(x = pvalue, col = stratum)) +
  stat_ecdf(geom = "step") + theme(legend.position = "bottom")
```

