---
title: '4'
output:
  html_document: default
  pdf_document: default
date: "2023-04-06"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 4  Mixture Models  

```{r}
## -----------------------------------------------------------------------------
coinflips = (runif(10000) > 0.5)
table(coinflips)
```

```{r}

oneFlip = function(fl, mean1 = 1, mean2 = 3, sd1 = 0.5, sd2 = 0.5) {
  if (fl) {
   rnorm(1, mean1, sd1)
  } else {
   rnorm(1, mean2, sd2)
  }
}
fairmix = vapply(coinflips, oneFlip, numeric(1))
library("ggplot2")
library("dplyr")
ggplot(tibble(value = fairmix), aes(x = value)) +
     geom_histogram(fill = "purple", binwidth = 0.1)

```

```{r}

## -----------------------------------------------------------------------------
means = c(1, 3)
sds   = c(0.5, 0.5)
values = rnorm(length(coinflips),
               mean = ifelse(coinflips, means[1], means[2]),
               sd   = ifelse(coinflips, sds[1],   sds[2]))



## -----------------------------------------------------------------------------
fair = tibble(
  coinflips = (runif(1e6) > 0.5),
  values = rnorm(length(coinflips),
                 mean = ifelse(coinflips, means[1], means[2]),
                 sd   = ifelse(coinflips, sds[1],   sds[2])))
ggplot(fair, aes(x = values)) +
     geom_histogram(fill = "purple", bins = 200)
```

```{r}

## -----------------------------------------------------------------------------
ggplot(dplyr::filter(fair, coinflips), aes(x = values)) +
  geom_histogram(aes(y = after_stat(density)), fill = "purple", binwidth = 0.01) +
  stat_function(fun = dnorm, color = "red",
                args = list(mean = means[1], sd = sds[1]))
```

```{r}


## -----------------------------------------------------------------------------
fairtheory = tibble(
  x = seq(-1, 5, length.out = 1000),
  f = 0.5 * dnorm(x, mean = means[1], sd = sds[1]) +
      0.5 * dnorm(x, mean = means[2], sd = sds[2]))
ggplot(fairtheory, aes(x = x, y = f)) +
  geom_line(color = "red", linewidth = 1.5) + ylab("mixture density")

```

```{r}

## -----------------------------------------------------------------------------
mystery = tibble(
  coinflips = (runif(1e3) > 0.5),
  values = rnorm(length(coinflips),
                 mean = ifelse(coinflips, 1, 2),
                 sd   = ifelse(coinflips, sqrt(.5), sqrt(.5))))
br2 = with(mystery, seq(min(values), max(values), length.out = 30))
ggplot(mystery, aes(x = values)) +
geom_histogram(fill = "purple", breaks = br2)

```

```{r}


## -----------------------------------------------------------------------------
head(mystery, 3)
br = with(mystery, seq(min(values), max(values), length.out = 30))
ggplot(mystery, aes(x = values)) +
  geom_histogram(data = dplyr::filter(mystery, coinflips),
     fill = "red", alpha = 0.2, breaks = br) +
  geom_histogram(data = dplyr::filter(mystery, !coinflips),
     fill = "darkblue", alpha = 0.2, breaks = br) 
```

```{r}


## -----------------------------------------------------------------------------
stopifnot(identical(br2, br))

## -----------------------------------------------------------------------------
ggplot(mystery, aes(x = values, fill = coinflips)) +
  geom_histogram(data = dplyr::filter(mystery, coinflips),
     fill = "red", alpha = 0.2, breaks = br) +
  geom_histogram(data = dplyr::filter(mystery, !coinflips),
     fill = "darkblue", alpha = 0.2, breaks = br) +
  geom_histogram(fill = "purple", breaks = br, alpha = 0.2)
```

```{r}

## -----------------------------------------------------------------------------
mus = c(-0.5, 1.5)
lambda = 0.5
u = sample(2, size = 100, replace = TRUE, prob = c(lambda, 1-lambda))
x = rnorm(length(u), mean = mus[u])
dux = tibble(u, x)
head(dux)
```

```{r}


## -----------------------------------------------------------------------------
group_by(dux, u) |> summarise(mu = mean(x), sigma = sd(x))
```

```{r}
table(dux$u) / nrow(dux)
```

```{r}


## -----------------------------------------------------------------------------
.o = options(digits = 3)
library("mixtools")
y = c(rnorm(100, mean = -0.2, sd = 0.5),
      rnorm( 50, mean =  0.5, sd =   1))
gm = normalmixEM(y, k = 2, 
                    lambda = c(0.5, 0.5),
                    mu = c(-0.01, 0.01), 
                    sigma = c(3, 3))
with(gm, c(lambda, mu, sigma, loglik))
options(.o)
```

```{r}
gm$lambda
```

```{r}

## -----------------------------------------------------------------------------
## PROVENANCE: here's a record of how the data were created
library("mosaics")
library("mosaicsExample")
for(f in c("wgEncodeSydhTfbsGm12878Stat1StdAlnRep1_chr22_sorted.bam",
           "wgEncodeSydhTfbsGm12878InputStdAlnRep1_chr22_sorted.bam"))
  constructBins(infile = system.file(file.path("extdata", f), package="mosaicsExample"),
    fileFormat = "bam", outfileLoc = "../data/",
    byChr = FALSE, useChrfile = FALSE, chrfile = NULL, excludeChr = NULL,
    PET = FALSE, fragLen = 200, binSize = 200, capping = 0)

datafiles = c("../data/wgEncodeSydhTfbsGm12878Stat1StdAlnRep1_chr22_sorted.bam_fragL200_bin200.txt",
              "../data/wgEncodeSydhTfbsGm12878InputStdAlnRep1_chr22_sorted.bam_fragL200_bin200.txt")
binTFBS = readBins(type = c("chip", "input"), fileName = datafiles)

## -----------------------------------------------------------------------------
binTFBS
```

```{r}



## -----------------------------------------------------------------------------
bincts = print(binTFBS)
ggplot(bincts, aes(x = tagCount)) +
  geom_histogram(binwidth = 1, fill = "forestgreen")
```

```{r}


## -----------------------------------------------------------------------------
ggplot(bincts, aes(x = tagCount)) + scale_y_log10() +
   geom_histogram(binwidth = 1, fill = "forestgreen")

```

```{r}

## -----------------------------------------------------------------------------
masses = c(A =  331, C =  307, G =  347, T =  322)
probs  = c(A = 0.12, C = 0.38, G = 0.36, T = 0.14)
N  = 7000
sd = 3
nuclt   = sample(length(probs), N, replace = TRUE, prob = probs)
quadwts = rnorm(length(nuclt),
                mean = masses[nuclt],
                sd   = sd)
ggplot(tibble(quadwts = quadwts), aes(x = quadwts)) +
  geom_histogram(bins = 100, fill = "purple")
```

```{r}


## -----------------------------------------------------------------------------
library("HistData")
ZeaMays$diff
```

```{r}
ggplot(ZeaMays, aes(x = diff, ymax = 1/15, ymin = 0)) +
  geom_linerange(linewidth = 1, col = "forestgreen") + ylim(0, 0.1)

## -----------------------------------------------------------------------------
stopifnot(nrow(ZeaMays) == 15)
```

```{r}


## -----------------------------------------------------------------------------
B = 1000
meds = replicate(B, {
  i = sample(15, 15, replace = TRUE)
  median(ZeaMays$diff[i])
})
ggplot(tibble(medians = meds), aes(x = medians)) +
  geom_histogram(bins = 30, fill = "purple")
```

```{r}


## -----------------------------------------------------------------------------
library("bootstrap")
bootstrap(ZeaMays$diff, B, mean)
bootstrap(ZeaMays$diff, B, median)
```
> Laplace distribution   

Probability density function :

$$f(x\mid \mu ,b)={\frac  {1}{2b}}\exp \left(-{\frac  {|x-\mu |}{b}}\right)\,\!$$ 

Laplace is a function of normal and exponential random variables:

$$X=\sqrt{W}\cdot Z, \quad W\sim Exp(1), \quad Z\sim N(0,1)$$ 
The Laplace distribution with mean $\mu \in \mathbb{R}$ and scale $b > 0$ has the probability density function   $$f_X(x)=\frac{1}{2b}\exp\left(-\frac{\left|x\right|}{b}\right)$$ 
Note that if we can show that the Laplace distribution with mean $0$ is a mixture of normals, then by shifting all these normals by $\mu$, it follows that the Laplace distribution with mean $\mu$ is also a mixture of normals. 

Fix $b > 0$. Let $W \sim \text{Exp}(\frac{1}{2b^2})$, i.e. $$f_W(w) = \dfrac{1}{2b^2} \exp \left(-\dfrac{x}{2b^2} \right)$$ for $w \geq 0$, and let $X \mid W = w \sim \mathcal{N}(0, w)$. We claim that $X$ has the Laplace distribution with mean $0$ and scale $b$. This is equivalent to showing that for any $x$,
$$\begin{align}
f_X(x)=\frac{1}{2b}\exp\left(-\frac{\left|x\right|}{b}\right)
\iff 
\int_{0}^{\infty} f_{X\mid W=w}(x)f_{W}(w)dw = \frac{1}{2b}\exp\left(-\frac{\left|x\right|}{b}\right)
\end{align}$$ 
$$\begin{align}
\int_{0}^{\infty} f_{X\mid W=w}(x)f_{W}(w)dw &= \int_{0}^{\infty} \frac{1}{\sqrt{2\pi w}}\exp\left(-\frac{x^2}{2w}\right)\frac{1}{2b^2}\exp\left(-\frac{w}{2b^2}\right)dw\\
&= \frac{1}{2b^2}\int_{0}^{\infty}\frac{1}{\sqrt{2\pi w}}\exp\left(-\frac{w^2+x^2b^2}{2b^2w}\right)dw\\
&= \frac{1}{2b^2}\int_{0}^{\infty}\frac{1}{\sqrt{2\pi w}}\exp\left(-\frac{(w-|x|b)^2}{2b^2w}-\frac{2w|x|b}{2b^2w}\right)dw\\
&= \frac{1}{2b^2}e^{-|x|/b}\int_{0}^{\infty}\frac{b}{\sqrt{2\pi b^2 w}}\exp\left(-\frac{(w-|x|b)^2}{2b^2w}\right)dw\\
&=\frac{1}{2b}\exp\left(-\frac{\left|x\right|}{b}\right)
\end{align}$$ 

The Mean of $$\frac{1}{2b}\exp\left(-\frac{\left|x-\mu\right|}{b}\right)$$ is $\mu$ and Variance is $2b^2$. 

The Laplace distribution is an example of where the consideration of the generative process indicates how the variance and mean are linked. The expectation value and variance of an asymmetric Laplace distribution $AL(\theta, \mu, \sigma)$ are $$\mathbb E(X)=\theta+ \mu$$ and $$\text{var} (X)=\mu^2 +\sigma^2$$ Note the variance is dependent on the mean, unless 
 (the case of the symmetric Laplace Distribution). This is the feature of the distribution that makes it useful. Having a mean-variance dependence is very common for physical measurements, be they microarray fluorescence intensities, peak heights from a mass spectrometer, or reads counts from high-throughput sequencing, as weâ€™ll see in the next section.


```{r}
## -----------------------------------------------------------------------------
c(N3 = choose(5, 3), N15 = choose(29, 15))

## -----------------------------------------------------------------------------
w = rexp(10000, rate = 1)

## -----------------------------------------------------------------------------
mu  = 0.3
lps = rnorm(length(w), mean = mu, sd = sqrt(w))
ggplot(data.frame(lps), aes(x = lps)) +
  geom_histogram(fill = "purple", binwidth = 0.1)
```

```{r}


## -----------------------------------------------------------------------------
mu = 0.3; sigma = 0.4; theta = -1
w  = rexp(10000, 1)
alps = rnorm(length(w), theta + mu * w, sigma * sqrt(w))
ggplot(tibble(alps), aes(x = alps)) +
  geom_histogram(fill = "purple", binwidth = 0.1)


```

```{r}


## -----------------------------------------------------------------------------
ggplot(tibble(x = rgamma(10000, shape = 2, rate = 1/3)),
   aes(x = x)) + geom_histogram(bins = 100, fill= "purple")
ggplot(tibble(x = rgamma(10000, shape = 10, rate = 3/2)),
   aes(x = x)) + geom_histogram(bins = 100, fill= "purple")
```

```{r}


## -----------------------------------------------------------------------------
lambda = rgamma(10000, shape = 10, rate = 3/2)
gp = rpois(length(lambda), lambda = lambda)
ggplot(tibble(x = gp), aes(x = x)) +
  geom_histogram(bins = 100, fill= "purple")

```

```{r}

## -----------------------------------------------------------------------------
library("vcd")
ofit = goodfit(gp, "nbinomial")
plot(ofit, xlab = "")
ofit$par
```

```{r}


## -----------------------------------------------------------------------------
x    = 0:95
mu   = 50
vtot = 80
v1   = vtot - mu
scale = v1/mu    # 0.6
shape = mu^2/v1  # 83.3
p1   = dgamma(x = x, scale = 0.6, shape = 80)
p2   = dpois(x = x, lambda = mu*1.2)
p3   = dnbinom(x = x, mu = mu, size = mu^2/vtot)

```

```{r}

## -----------------------------------------------------------------------------
library("RColorBrewer")
cols = brewer.pal(8, "Paired")
par(mfrow=c(3,1), mai=c(0.5, 0.5, 0.01, 0.01))
xlim = x[c(1, length(x))]
plot(NA, NA, xlim=xlim, ylim=c(0,0.07), type="n", ylab="", xlab="")
polygon(x, p1, col=cols[1])
abline(v=mu, col="black", lwd=3)
abline(v=mu*1.2, col=cols[2], lty=2, lwd=3)
plot(x, p2, col=cols[3], xlim=xlim, ylab="", xlab="", type="h", lwd=2)
abline(v=mu*1.2, col=cols[2], lwd=2)
abline(v=mu*1.1, col=cols[4], lty=2, lwd=3)
plot(x, p3, col=cols[4], xlim=xlim, type="h", lwd=2, ylab="", xlab="")
```

```{r}


## -----------------------------------------------------------------------------
lambdas = seq(100, 900, by = 100)
simdat = lapply(lambdas, function(l)
    tibble(y = rpois(n = 40, lambda=l), lambda = l)
  ) %>% bind_rows
simdat
```

```{r}
library("ggbeeswarm")
ggplot(simdat, aes(x = lambda, y = y)) +
  geom_beeswarm(alpha = 0.6, color = "purple")
```

```{r}
ggplot(simdat, aes(x = lambda, y = sqrt(y))) +
  geom_beeswarm(alpha = 0.6, color = "purple")
```

[Expected Value of Square Root of Poisson Random Variable](https://math.stackexchange.com/questions/1536459/expected-value-of-square-root-of-poisson-random-variable)

In general, for smooth $g(X)$ you can do a Taylor expansion around the mean $\mu=E(X)$ :

$$g(X)=g(\mu)+gâ€²(\mu)(Xâˆ’\mu)+\frac{gâ€²â€²(\mu)}{2!}(Xâˆ’\mu)^2+\frac{gâ€²â€²â€²(\mu)}{3!}(Xâˆ’\mu)^3+â‹¯$$
So

$$E[g(X)]=g(\mu)+\frac{gâ€²â€²(\mu)}{2!}m_2+\frac{gâ€²â€²â€²(\mu)}{3!}m_3+â‹¯$$
where $m_i$ is the $i^{th}$ centered moment. In our case $m_2=m_3=\lambda$, so:

$$E[g(X)]=\sqrt{\lambda}âˆ’\frac{\lambda^{âˆ’1/2}}{8}+\frac{\lambda^{âˆ’3/2}}{16}+â‹¯$$
This approximation is useful only if $\lambda \gg 1$

The mean and variance of any Poisson process is given as 

$$E[P(\lambda_i)] = Var[P(\lambda_i)] = \lambda_i$$

[Square root transformation of Poisson process](https://stats.stackexchange.com/questions/505298/square-root-transformation-of-poisson-process-how-small-var-sqrtp-lambda)

$$\operatorname{var}(\sqrt{P(\lambda)})=E[P(\lambda)]-E[\sqrt{P(\lambda)}]^2=\lambda-E[\sqrt{P(\lambda)}]^2$$

The second term can be approximated *better* as follows:

$$E[\sqrt{P(\lambda)}]\approx \sqrt{\lambda}-\frac{\lambda^{-1/2}}{8}+\frac{\lambda^{-3/2}}{16}+...$$

Which is why the expected value is also approximated by $\sqrt \lambda$. Square of it will be
$$E[\sqrt{P(\lambda)}]^2\approx \lambda -\frac{1}{4}+\frac{9}{64\lambda}+...$$

So, the variance will be approximately
$$\operatorname{var}(\sqrt{P(\lambda)})\approx \frac{1}{4}-\frac{9}{64\lambda}+...$$

which is approximately $1/4$ for large $\lambda$, and the standard deviation is approximately $1/2$.


```{r}


## -----------------------------------------------------------------------------
.o = options(digits = 3)
summarise(group_by(simdat, lambda), sd(y), sd(2*sqrt(y)))
options(.o)
```

```{r}


## -----------------------------------------------------------------------------
muvalues = 2^seq(0, 10, by = 1)
simgp = lapply(muvalues, function(mu) {
  u = rnbinom(n = 1e4, mu = mu, size = 4) # size:	target for number of successful trials, or dispersion parameter (the shape parameter of the gamma mixing distribution). Must be strictly positive, need not be integer.
  tibble(mean = mean(u), sd = sd(u),
         lower = quantile(u, 0.025),
         upper = quantile(u, 0.975),
         mu = mu)
  } ) %>% bind_rows
simgp

```

```{r}
head(as.data.frame(simgp), 2)
```

```{r}
ggplot(simgp, aes(x = mu, y = mean, ymin = lower, ymax = upper)) +
  geom_point() + geom_errorbar()
```

```{r}


## -----------------------------------------------------------------------------
simgp = mutate(simgp,
  slopes = 1 / sd,
  trsf   = cumsum(slopes * mean))
ggplot(simgp, aes(x = mean, y = trsf)) +
  geom_point() + geom_line() + xlab("")
```


Call our transformation function $g$, and assume itâ€™s differentiable. Also call our random variables $X_i$ , with means $\mu_i$ and variances $v_i$ , and we assume that 
$\mu_i$ and $v_i$ are related by a functional relationship $v_i = \nu(\mu_i)$ . Then, for values of $X_i$ in the neighborhood of its mean $\mu_i$, 
$$g(X_i)=g(\mu_i)+g'(\mu_i)(X_i-\mu_i)+\cdots$$ 

where the dots stand for higher order terms that we can neglect. The variances of the transformed values are then 
$$\text{Var }(g(X_i))\simeq g'(\mu_i)^2$$ 
$$\text{Var }(X_i)\simeq g'(\mu_i)^2\nu(\mu_i)$$ 

where we have used the rules $\text{Var}(X-c)=\text{Var}(X)$ and $\text{Var}(cX)=c^2\text{Var}(X)$ that hold whenever $c$
 is a constant number. Requiring that this be constant leads to the differential equation $$g'(x)=\frac{1}{\sqrt{\nu(x)}}$$  For a given mean-variance relationship $\nu(\mu)$, we can solve this for the function $g$ . Letâ€™s check this for some simple cases: 
 
> - if $\nu(\mu)=\mu$(Poisson), we recover $g(x)=\sqrt{x}$, the square root transformation.

> - If $\nu(\mu)=\alpha \mu^2$, solving the differential equation $$g'(x)=\frac{1}{\sqrt{\nu(x)}}$$ gives $g(x)=\log(x)$. This explains why the logarithm transformation is so popular in many data analysis applications: it acts as a variance stabilizing transformation whenever the data have a constant coefficient of variation, that is, when the standard deviation is proportional to the mean.



```{r}


## -----------------------------------------------------------------------------
f = function(x, a) 
  ifelse (a==0, 
    sqrt(x), 
    log(2*sqrt(a) * sqrt(x*(a*x+1)) + 2*a*x+1) / (2*sqrt(a)))
x  = seq(0, 24, by = 0.1)
df = lapply(c(0, 0.05*2^(0:5)), function(a) 
  tibble(x = x, a = a, y = f(x, a))) %>% bind_rows()
ggplot(df, aes(x = x, y = y, col = factor(a))) + 
  geom_line() + labs(col = expression(alpha))
```

```{r}


## -----------------------------------------------------------------------------
f2 = function(x, a) ifelse (a==0, sqrt(x), acosh(2*a*x + 1) / (2*sqrt(a)))  
with(df, max(abs(f2(x,a) - y)))
stopifnot(with(df, max(abs(f2(x,a) - y))) < 1e-10)
```

```{r}

## -----------------------------------------------------------------------------
a = c(0.2, 0.5, 1)
f(1e6, a) 
1/(2*sqrt(a)) * (log(1e6) + log(4*a))
```

```{r}

## -----------------------------------------------------------------------------
mx = readRDS("../data/Myst.rds")$yvar
str(mx)
```

```{r}
ggplot(tibble(mx), aes(x = mx)) + geom_histogram(binwidth = 0.025)
```

```{r}



## -----------------------------------------------------------------------------
wA = runif(length(mx))
wB = 1 - wA

## -----------------------------------------------------------------------------
iter      = 0
loglik    = -Inf
delta     = +Inf
tolerance = 1e-3
miniter   = 50
maxiter   = 1000
```

```{r}
## -----------------------------------------------------------------------------
while((delta > tolerance) && (iter <= maxiter) || (iter < miniter)) {
  lambda = mean(wA)
  muA = weighted.mean(mx, wA)
  muB = weighted.mean(mx, wB)
  sdA = sqrt(weighted.mean((mx - muA)^2, wA))
  sdB = sqrt(weighted.mean((mx - muB)^2, wB))

  pA   =    lambda    * dnorm(mx, mean = muA, sd = sdA)
  pB   = (1 - lambda) * dnorm(mx, mean = muB, sd = sdB)
  ptot = pA + pB
  wA   = pA / ptot
  wB   = pB / ptot

  loglikOld = loglik
  loglik = sum(log(pA)) + sum(log(pB))
  delta = abs(loglikOld - loglik)
  iter = iter + 1
}
iter
```

```{r}



## -----------------------------------------------------------------------------
.o = options(digits = 3)
c(lambda, muA, muB, sdA, sdB)
options(.o)

## -----------------------------------------------------------------------------
.o = options(digits = 3)
gm = mixtools::normalmixEM(mx, k = 2)
with(gm, c(lambda[1], mu, sigma))
options(.o)

```

```{r}

## -----------------------------------------------------------------------------
library("flexmix")
data("NPreg")
NPreg

```
```{r}
## -----------------------------------------------------------------------------
m1 = flexmix(yn ~ x + I(x^2), data = NPreg, k = 2)

## -----------------------------------------------------------------------------
ggplot(NPreg, aes(x = x, y = yn)) + geom_point()
```

```{r}
## -----------------------------------------------------------------------------
modeltools::parameters(m1, component = 1)
```

```{r}

modeltools::parameters(m1, component = 2)
```

```{r}
modeltools::clusters(m1)
```

```{r}
## -----------------------------------------------------------------------------
table(NPreg$class, mode ltools::clusters(m1))

```

```{r}
## -----------------------------------------------------------------------------
summary(m1)
```

```{r}



## -----------------------------------------------------------------------------
NPreg = mutate(NPreg, gr = factor(class))
ggplot(NPreg, aes(x = x, y = yn, group = gr)) +
   geom_point(aes(colour = gr, shape = gr)) +
   scale_colour_hue(l = 40, c = 180)

```

